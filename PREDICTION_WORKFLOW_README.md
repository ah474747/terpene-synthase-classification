# Real-World Germacrene Synthase Discovery Workflow

## ðŸŽ¯ Overview

This workflow enables you to discover novel germacrene synthases from UniProt by combining:
- **Local processing** (CPU): Sequence download, model prediction, analysis
- **Google Colab** (GPU): ESM-2 embedding generation only

**Total runtime**: ~1-2 hours for 5,000 sequences
- Step 1 (local): ~10 minutes
- Step 2 (Colab): ~30-60 minutes  
- Step 3 (local): ~5 minutes

---

## ðŸ“‹ Prerequisites

### Local Environment (MacBook Pro)
```bash
# Already installed from requirements.txt
pip install numpy pandas scikit-learn xgboost matplotlib seaborn joblib biopython requests tqdm
```

### Google Colab
- Free Google account
- GPU runtime enabled (Runtime > Change runtime type > GPU)
- No local installation needed (packages installed in notebook)

---

## ðŸš€ Three-Step Workflow

### **Step 1: Download UniProt Sequences (Local - 10 min)**

Download putative terpene synthase sequences from UniProt, excluding MARTS-DB training sequences.

```bash
cd /Users/andrewhorwitz/Documents/Cursor_AI_projects/Terpene_Stuff/terpene-synthase-classification

python3 scripts/step1_download_uniprot_tps.py --max_sequences 5000
```

**What it does:**
- Downloads ~1,200 REVIEWED terpene synthase sequences from UniProt
- Fills to target (5,000) with UNREVIEWED sequences
- Query: terpene/monoterpene/sesquiterpene/diterpene synthases, 200-1500 aa, no fragments
- Excludes sequences already in MARTS-DB training set
- Saves: `data/uniprot_tps_sequences.fasta` and `data/uniprot_tps_metadata.csv`

**Expected output:**
```
Downloading reviewed sequences...
Downloaded 1285 sequences
Parsed 1285 reviewed sequences
Need 3738 more sequences from unreviewed entries...
Downloading unreviewed sequences...
Downloaded 8943 unreviewed sequences
Total after adding unreviewed: 5023

Filtering sequences by MARTS-DB exclusion list...
Excluded 23 sequences (already in MARTS-DB)
Retained 5000 novel sequences

FASTA file: data/uniprot_tps_sequences.fasta
Metadata file: data/uniprot_tps_metadata.csv
```

---

### **Step 2: Generate ESM-2 Embeddings (Google Colab - 30-60 min)**

Generate protein embeddings using ESM-2 model on GPU.

**Instructions:**

1. **Open Google Colab**: https://colab.research.google.com/

2. **Upload notebook**: 
   - Click "File > Upload notebook"
   - Select `notebooks/step2_generate_embeddings_colab.ipynb`

3. **Enable GPU**:
   - Runtime > Change runtime type > GPU (T4)

4. **Upload FASTA file**:
   - Click folder icon (ðŸ“) on left sidebar
   - Upload `data/uniprot_tps_sequences.fasta` from Step 1

5. **Run all cells**:
   - Runtime > Run all (or Ctrl+F9)
   - Wait ~30-60 minutes for completion

6. **Download embeddings**:
   - Right-click `uniprot_tps_embeddings.npy` in file browser
   - Select "Download"
   - Move to your local `data/` directory

**Expected output in Colab:**
```
âœ“ GPU available: Tesla T4
âœ“ Model loaded on cuda
âœ“ Loaded 5000 sequences
Processing sequences: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000
âœ“ Generated embeddings shape: (5000, 1280)
âœ“ Embeddings saved!
```

**Important**: The embeddings file will be ~25 MB. Make sure to download it!

---

### **Step 3: Predict Germacrene Synthases (Local - 5 min)**

Load trained XGBoost model and generate confidence-ranked predictions.

```bash
python3 scripts/step3_predict_germacrene.py
```

**What it does:**
- Loads embeddings from Step 2
- Loads trained germacrene XGBoost model
- Generates predictions with confidence scores
- Ranks sequences by confidence (highest = most likely germacrene)
- Creates visualizations and multiple output files

**Expected output:**
```
Loading embeddings from data/uniprot_tps_embeddings.npy
Loading model from colab_upload/germacrene_xgboost_model.pkl
Generating predictions...
Predicted positive: 347
Mean confidence: 0.423

GERMACRENE SYNTHASE PREDICTION SUMMARY
===============================================================================
Total sequences analyzed: 5000
Predicted germacrene synthases: 347

CONFIDENCE THRESHOLDS
  >0.90:   23 sequences
  >0.80:   67 sequences  â† YOUR BEST TARGETS
  >0.70:  142 sequences
  >0.60:  278 sequences

TOP 10 HIGHEST CONFIDENCE PREDICTIONS
  1. Q9XYZ1 (Salvia officinalis)
     Confidence: 0.9423
     Protein: Terpene synthase

  2. P12345 (Artemisia annua)
     Confidence: 0.9187
     ...
```

---

## ðŸ“Š Output Files

All files saved to `results/` directory:

### 1. **all_predictions_ranked.csv**
Complete ranked list of all sequences with confidence scores.

| rank | uniprot_id | organism | confidence | prediction | protein_name |
|------|------------|----------|------------|------------|--------------|
| 1 | Q9XYZ1 | Salvia officinalis | 0.9423 | germacrene | Terpene synthase |
| 2 | P12345 | Artemisia annua | 0.9187 | germacrene | Sesquiterpene synthase |
| ... | ... | ... | ... | ... | ... |

### 2. **top_100_predictions.csv**
Top 100 highest confidence predictions - your best experimental targets.

### 3. **high_confidence_predictions.csv**
All sequences with confidence >0.80 - priority validation targets.

### 4. **confidence_distribution.png**
Visualization showing:
- Histogram of confidence scores
- Confidence by prediction class
- Cumulative distribution
- Top 100 bar chart

### 5. **prediction_summary.json**
Summary statistics in machine-readable format.

---

## ðŸŽ¯ How to Use Results

### **Experimental Validation Strategy**

**Tier 1: High Confidence (>0.80)** - 50-100 sequences
- **Success rate**: 70-90%
- **Action**: Priority targets for cloning/expression
- **Expected**: 35-90 validated germacrene synthases

**Tier 2: Medium-High (0.65-0.80)** - 100-200 sequences  
- **Success rate**: 50-70%
- **Action**: Secondary targets, combine with literature
- **Expected**: Novel variants, different organisms

**Tier 3: Medium (0.50-0.65)** - 200-400 sequences
- **Success rate**: 30-50%
- **Action**: Deprioritize unless other evidence supports

### **Selection Criteria**

When choosing sequences for validation:

1. **Confidence score** (primary criterion)
   - Start with confidence >0.80

2. **Organism of interest**
   - Focus on specific plant families
   - Target agricultural/medicinal species

3. **Novel/uncharacterized proteins**
   - "Hypothetical protein" = discovery opportunity
   - Unreviewed UniProt entries

4. **Sequence diversity**
   - Avoid redundancy (check organism/gene name)
   - Sample across plant families

5. **Experimental feasibility**
   - Gene synthesis costs
   - Expression system compatibility

### **Example Selection Process**

```python
import pandas as pd

# Load results
results = pd.read_csv('results/all_predictions_ranked.csv')

# Filter high confidence
high_conf = results[results['confidence'] > 0.80]

# Focus on specific organism family (e.g., Asteraceae)
asteraceae = high_conf[high_conf['organism'].str.contains('Artemisia|Lactuca|Helianthus', case=False)]

# Get top 20 for validation
targets = asteraceae.head(20)

# Export for lab
targets.to_csv('experimental_targets.csv', index=False)
```

---

## ðŸ”¬ Model Performance Expectations

Based on benchmark results from manuscript:

**Germacrene XGBoost Model:**
- F1-score: 0.744 Â± 0.028
- AUC-ROC: 0.894 Â± 0.017
- AUC-PR: 0.680 Â± 0.039
- Precision at high confidence (>0.80): ~70-90%

**What this means for real-world discovery:**

If you validate **50 high-confidence predictions**:
- **Expected successes**: 35-45 true germacrene synthases
- **Expected false positives**: 5-15 non-germacrene enzymes
- **Discovery value**: These false positives might produce other interesting terpenes!

---

## âš™ï¸ Customization Options

### **Adjust Number of Sequences**

Download more or fewer sequences:
```bash
python3 scripts/step1_download_uniprot_tps.py --max_sequences 10000
```

### **Change Confidence Threshold**

In Step 3, edit the threshold in the code:
```python
high_conf = results[results['confidence'] > 0.75]  # Lower threshold
```

### **Filter by Organism**

Add organism filtering to Step 1:
```python
# In step1_download_uniprot_tps.py, modify query:
query = (
    '(keyword:"terpene synthase") '
    'AND (organism_id:33090) '  # Viridiplantae
    'AND (taxonomy_name:"Asteraceae") '  # Add family filter
    'AND (fragment:false) '
    'AND (length:[400 TO 1000])'
)
```

### **Use Different Model**

Train on different terpene (pinene, myrcene):
```bash
# In step3_predict_germacrene.py
python3 step3_predict_pinene.py --model colab_upload/pinene_xgboost_model.pkl
```

---

## ðŸ› Troubleshooting

### **Step 1: UniProt Download Issues**

**Problem**: `ConnectionError` or timeout
```bash
# Solution: Try with smaller batches
python3 scripts/step1_download_uniprot_tps.py --max_sequences 1000
```

**Problem**: No sequences downloaded
```bash
# Solution: Check UniProt API status
curl "https://rest.uniprot.org/uniprotkb/search?query=terpene&size=1"
```

### **Step 2: Colab Issues**

**Problem**: "No GPU available"
- Solution: Runtime > Change runtime type > GPU

**Problem**: "Out of memory"
- Solution: Restart runtime, reduce batch size in notebook

**Problem**: Embeddings contain NaN
- Solution: Check sequences for special characters, re-run

### **Step 3: Prediction Issues**

**Problem**: `FileNotFoundError: uniprot_tps_embeddings.npy`
```bash
# Solution: Make sure you downloaded embeddings from Colab
ls -lh data/uniprot_tps_embeddings.npy
```

**Problem**: Shape mismatch errors
```bash
# Solution: Verify embeddings and metadata have same number of sequences
python3 -c "import numpy as np; import pandas as pd; print(np.load('data/uniprot_tps_embeddings.npy').shape); print(len(pd.read_csv('data/uniprot_tps_metadata.csv')))"
```

---

## ðŸ“Š Expected Resource Usage

### **Local (MacBook Pro 16GB RAM)**
- Memory: ~2-4 GB
- Disk space: ~50 MB (FASTA + embeddings + results)
- Time: ~15 minutes total

### **Google Colab (Free Tier)**
- GPU: Tesla T4 (15 GB VRAM)
- Runtime: ~30-60 minutes
- Memory: ~8 GB RAM
- GPU usage: ~60% during embedding generation

---

## ðŸ“š Citation

If you use this workflow in your research, please cite:

```bibtex
@article{terpene-classification-2024,
  title={Machine Learning Outperforms Traditional Bioinformatics for Terpene Synthase Classification},
  author={[Your Name]},
  journal={[Journal]},
  year={2024},
  note={GitHub: https://github.com/ah474747/terpene-synthase-classification}
}
```

---

## ðŸ†˜ Need Help?

1. Check this README for troubleshooting tips
2. Review script documentation (`--help` flag)
3. Check GitHub Issues: https://github.com/ah474747/terpene-synthase-classification/issues
4. Verify all dependencies are installed: `pip list`

---

## âœ… Quick Start Checklist

- [ ] Install local dependencies (`pip install -r requirements.txt`)
- [ ] Run Step 1: Download sequences
- [ ] Upload FASTA to Google Colab
- [ ] Run Step 2: Generate embeddings (enable GPU!)
- [ ] Download embeddings from Colab
- [ ] Run Step 3: Generate predictions
- [ ] Review `top_100_predictions.csv`
- [ ] Select candidates for experimental validation
- [ ] Update your lab notebook with targets
- [ ] Validate in the wet lab!

**ðŸš€ Good luck discovering novel germacrene synthases!**

