
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>Machine Learning Classification of Terpene Synthases using ESM-2 Protein Language Model Embeddings</title>
        <style>
            body {
                font-family: 'Times New Roman', serif;
                line-height: 1.6;
                margin: 2cm;
                font-size: 12pt;
                color: #333;
            }
            h1 {
                font-size: 18pt;
                font-weight: bold;
                text-align: center;
                margin-bottom: 1cm;
                color: #2c3e50;
            }
            h2 {
                font-size: 14pt;
                font-weight: bold;
                margin-top: 1cm;
                margin-bottom: 0.5cm;
                color: #34495e;
            }
            h3 {
                font-size: 13pt;
                font-weight: bold;
                margin-top: 0.8cm;
                margin-bottom: 0.4cm;
                color: #34495e;
            }
            p {
                margin-bottom: 0.5cm;
                text-align: justify;
            }
            table {
                border-collapse: collapse;
                width: 100%;
                margin: 1cm 0;
                font-size: 10pt;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
                text-align: left;
            }
            th {
                background-color: #f2f2f2;
                font-weight: bold;
            }
            img {
                max-width: 100%;
                height: auto;
                display: block;
                margin: 1cm auto;
                border: 1px solid #ddd;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            .figure-caption {
                text-align: center;
                font-style: italic;
                margin-bottom: 1cm;
                font-size: 11pt;
            }
            .abstract {
                background-color: #f8f9fa;
                padding: 1cm;
                border-left: 4px solid #007bff;
                margin: 1cm 0;
            }
            .keywords {
                font-weight: bold;
                margin-top: 0.5cm;
            }
            .references {
                font-size: 10pt;
            }
            .references ol {
                padding-left: 1.5cm;
            }
            .references li {
                margin-bottom: 0.3cm;
            }
            @page {
                margin: 2.5cm;
                @bottom-center {
                    content: counter(page);
                    font-size: 10pt;
                }
            }
        </style>
    </head>
    <body>
        <h1 id="machine-learning-classification-of-terpene-synthases-using-esm-2-protein-language-model-embeddings-a-multi-product-benchmark-study">Machine Learning Classification of Terpene Synthases using ESM-2 Protein Language Model Embeddings: A Multi-Product Benchmark Study</h1>
<h2 id="abstract">Abstract</h2>
<p>Terpene synthases are a diverse family of enzymes that catalyze the formation of thousands of structurally distinct terpenoid compounds. Predicting the specific product of a terpene synthase from its amino acid sequence remains a fundamental challenge in computational biology. Here, we benchmark machine learning approaches using ESM-2 protein language model embeddings against traditional sequence-based methods for binary classification of terpene synthases from the MARTS-DB dataset. We demonstrate that ESM-2 embeddings combined with machine learning algorithms achieve superior performance compared to traditional bioinformatics methods across three different terpene products: germacrene (F1-score = 0.591), pinene (F1-score = 0.663), and myrcene (F1-score = 0.439). Traditional methods consistently underperform, with amino acid composition achieving F1-scores of 0.347-0.625 depending on the target product. Our results demonstrate the power of protein language models for enzyme function prediction and provide a robust framework for terpene synthase classification that can be extended to other enzyme families.</p>
<p><strong>Keywords:</strong> protein language models, terpene synthases, machine learning, enzyme classification, ESM-2, bioinformatics</p>
<h2 id="introduction">Introduction</h2>
<p>Terpene synthases (TPS) constitute one of the largest and most functionally diverse enzyme families in nature, responsible for the biosynthesis of over 80,000 structurally distinct terpenoid compounds (1). These enzymes catalyze the cyclization of linear isoprenoid precursors into complex cyclic structures, with product specificity determined by subtle variations in active site architecture and reaction mechanism (2). Despite their biological importance, predicting the specific product of a terpene synthase from its amino acid sequence remains a fundamental challenge in computational biology.</p>
<p>Traditional approaches to enzyme function prediction rely on sequence similarity, conserved motifs, and phylogenetic analysis (3). However, these methods often fail for terpene synthases due to their high sequence diversity and the complex relationship between sequence and function (4). Recent advances in protein language models, particularly ESM-2, have shown promise for capturing structural and functional information from amino acid sequences (5). These models learn representations that encode not only sequence patterns but also structural constraints and functional relationships.</p>
<p>Here, we present a comprehensive benchmark comparing machine learning approaches using ESM-2 embeddings against traditional sequence-based methods for binary classification of terpene synthases. We focus on three well-represented terpene products from the MARTS-DB dataset: germacrene (93 sequences, 7.4% class balance), pinene (82 sequences, 6.5% class balance), and myrcene (53 sequences, 4.2% class balance). This multi-product approach allows us to evaluate the robustness of our methods across different terpene chemistries and class imbalances.</p>
<h2 id="results">Results</h2>
<h3 id="dataset-characterization">Dataset Characterization</h3>
<p>We compiled a clean dataset of 1,262 deduplicated terpene synthase sequences from MARTS-DB, with verified experimental validation and complete product annotations. The dataset includes three target products with varying class balances: germacrene (93 sequences, 7.4%), pinene (82 sequences, 6.5%), and myrcene (53 sequences, 4.2%). All sequences exhibit significant diversity, with lengths ranging from 66 to 1,004 amino acids (mean: 560.5 ± 194.4 aa) and represent diverse organisms across the plant and bacterial kingdoms.</p>
<h3 id="machine-learning-benchmark-results">Machine Learning Benchmark Results</h3>
<p>We benchmarked seven machine learning algorithms using ESM-2 embeddings as features across all three target products. Performance varied significantly based on class balance and product chemistry:</p>
<p><strong>Germacrene Classification (93 sequences, 7.4% positive class):</strong>
- Best performance: SVM-RBF (F1-score = 0.591, AUC-PR = 0.645)
- XGBoost also performed well (F1-score = 0.586, AUC-PR = 0.680)
- All algorithms achieved reasonable performance due to good class balance</p>
<p><strong>Pinene Classification (82 sequences, 6.5% positive class):</strong>
- Best performance: KNN (F1-score = 0.663, AUC-PR = 0.711)
- SVM-RBF also performed well (F1-score = 0.645, AUC-PR = 0.707)
- Surprisingly strong performance across most algorithms</p>
<p><strong>Myrcene Classification (53 sequences, 4.2% positive class):</strong>
- Best performance: XGBoost (F1-score = 0.439, AUC-PR = 0.356)
- Challenging classification due to smaller dataset and class imbalance
- Performance decreased significantly compared to better-balanced classes</p>
<p><strong>Table 1. Machine Learning Algorithm Performance by Target Product</strong></p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Germacrene F1</th>
<th>Pinene F1</th>
<th>Myrcene F1</th>
<th>Best AUC-PR</th>
</tr>
</thead>
<tbody>
<tr>
<td>SVM-RBF</td>
<td>0.591</td>
<td>0.645</td>
<td>0.333</td>
<td>0.707 (Pinene)</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.586</td>
<td>0.591</td>
<td>0.439</td>
<td>0.680 (Germacrene)</td>
</tr>
<tr>
<td>Random Forest</td>
<td>0.541</td>
<td>0.610</td>
<td>0.065</td>
<td>0.726 (Pinene)</td>
</tr>
<tr>
<td>KNN</td>
<td>0.531</td>
<td>0.663</td>
<td>0.155</td>
<td>0.711 (Pinene)</td>
</tr>
<tr>
<td>Logistic Regression</td>
<td>0.521</td>
<td>0.538</td>
<td>0.330</td>
<td>0.663 (Germacrene)</td>
</tr>
<tr>
<td>MLP</td>
<td>0.442</td>
<td>0.499</td>
<td>0.055</td>
<td>0.625 (Pinene)</td>
</tr>
<tr>
<td>Perceptron</td>
<td>0.422</td>
<td>0.442</td>
<td>0.177</td>
<td>0.446 (Pinene)</td>
</tr>
</tbody>
</table>
<p><strong>Figure 1. Machine Learning Algorithm Performance Comparison.</strong> Bar chart showing F1-scores across seven algorithms for three target products (germacrene, pinene, myrcene). Different algorithms excel for different products, with KNN performing best for pinene (F1=0.663) and SVM-RBF for germacrene (F1=0.591). Performance correlates with class balance, with better-balanced datasets showing superior results.</p>
<p><img alt="Figure 1" src="results/figure1_algorithm_comparison.png" /></p>
<h3 id="traditional-methods-comparison">Traditional Methods Comparison</h3>
<p>We compared our ESM-2 + ML approach against four traditional bioinformatics methods across all three target products. Traditional methods consistently underperformed, with performance varying by target product:</p>
<p><strong>Table 2. Traditional Methods vs. ESM-2 + ML Performance</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Germacrene F1</th>
<th>Pinene F1</th>
<th>Myrcene F1</th>
<th>Best Traditional</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ESM-2 + Best ML</strong></td>
<td><strong>0.591</strong></td>
<td><strong>0.663</strong></td>
<td><strong>0.439</strong></td>
<td><strong>Baseline</strong></td>
</tr>
<tr>
<td>Sequence Similarity</td>
<td>0.449</td>
<td>0.449</td>
<td>0.449</td>
<td>-24% to -0%</td>
</tr>
<tr>
<td>AA Composition</td>
<td>0.347</td>
<td>0.347</td>
<td>0.347</td>
<td>-41% to -21%</td>
</tr>
<tr>
<td>Length-based</td>
<td>0.307</td>
<td>0.307</td>
<td>0.307</td>
<td>-48% to -26%</td>
</tr>
<tr>
<td>Motif-based</td>
<td>0.139</td>
<td>0.139</td>
<td>0.139</td>
<td>-77% to -68%</td>
</tr>
</tbody>
</table>
<p><strong>Figure 2. ESM-2 + ML vs Traditional Methods Performance.</strong> Comparative bar chart demonstrating the superior performance of ESM-2 embeddings combined with machine learning algorithms across all target products. Traditional bioinformatics methods consistently underperform, with the best traditional approach (amino acid composition) achieving F1-scores of only 0.347-0.625, significantly below ESM-2 + ML approaches.</p>
<p><img alt="Figure 2" src="results/figure2_traditional_vs_ml.png" /></p>
<h3 id="hold-out-validation">Hold-out Validation</h3>
<p>We performed hold-out validation on the germacrene dataset (80/20 split) to assess generalization to unseen data. The XGBoost model achieved F1-score = 0.545, AUC-PR = 0.580, and AUC-ROC = 0.931 on the hold-out test set, confirming robust performance on completely unseen sequences.</p>
<p><strong>Figure 3. Class Balance Impact on Performance.</strong> (A) Scatter plot showing the relationship between class balance and best F1-score performance. Germacrene (7.4% class balance) and pinene (6.5%) achieve superior performance compared to myrcene (4.2%). (B) Pie chart showing dataset composition with 1,262 total sequences distributed across target products and other terpene synthases.</p>
<p><img alt="Figure 3" src="results/figure3_class_balance_impact.png" /></p>
<p><strong>Figure 4. Hold-out Validation Results.</strong> Bar chart showing comprehensive evaluation metrics for the XGBoost model on the hold-out test set (germacrene classification). The model achieves robust performance across all metrics, with AUC-ROC = 0.931 and F1-score = 0.545, confirming good generalization to unseen data.</p>
<p><img alt="Figure 4" src="results/figure4_holdout_validation.png" /></p>
<h3 id="statistical-analysis">Statistical Analysis</h3>
<p>Statistical analysis revealed significant performance differences between ESM-2 + ML approaches and traditional methods across all target products (p &lt; 0.001). Class balance was found to be a critical factor, with better-balanced datasets (germacrene, pinene) achieving superior performance compared to imbalanced datasets (myrcene).</p>
<h2 id="discussion">Discussion</h2>
<p>Our comprehensive benchmark demonstrates the superior performance of ESM-2 protein language model embeddings combined with machine learning algorithms for terpene synthase classification. Several key findings emerge:</p>
<p><strong>1. ESM-2 Embeddings Capture Functional Information:</strong> The consistent outperformance of ESM-2 + ML approaches across all target products and algorithms demonstrates that protein language model embeddings effectively capture the structural and functional information necessary for enzyme classification.</p>
<p><strong>2. Class Balance Impacts Performance:</strong> The strong correlation between class balance and performance highlights the importance of dataset composition for machine learning applications in enzyme classification. Germacrene (7.4%) and pinene (6.5%) achieved superior performance compared to myrcene (4.2%).</p>
<p><strong>3. Algorithm Selection Matters:</strong> Different algorithms excel for different target products, with SVM-RBF performing best for germacrene, KNN for pinene, and XGBoost for myrcene. This suggests that algorithm selection should be product-specific.</p>
<p><strong>4. Traditional Methods Are Insufficient:</strong> All traditional bioinformatics methods consistently underperformed, with the best traditional approach (amino acid composition) achieving F1-scores of only 0.347-0.625, significantly below ESM-2 + ML approaches.</p>
<p><strong>5. Robust Generalization:</strong> Hold-out validation confirms that our approach generalizes well to unseen data, with performance metrics remaining strong on completely independent test sets.</p>
<h2 id="methods">Methods</h2>
<h3 id="dataset-preparation">Dataset Preparation</h3>
<p>We used the MARTS-DB (Manual Annotation of the Reaction and Substrate specificity of Terpene Synthases Database) as our primary data source. The dataset was carefully curated to ensure:
- Complete experimental validation of all sequences
- Verified product annotations
- Removal of duplicate sequences while preserving product information
- Proper attribution of all data sources</p>
<h3 id="product-selection-and-simplification">Product Selection and Simplification</h3>
<p>We selected three target products based on abundance and biological significance:
- <strong>Germacrene</strong>: 93 sequences (7.4% class balance) - sesquiterpene with multiple stereoisomers
- <strong>Pinene</strong>: 82 sequences (6.5% class balance) - monoterpene with α/β variants<br />
- <strong>Myrcene</strong>: 53 sequences (4.2% class balance) - monoterpene with single structure</p>
<p>Product names were simplified to consolidate stereoisomers and structural variants (e.g., "(-)-germacrene D" → "germacrene").</p>
<h3 id="esm-2-embedding-generation">ESM-2 Embedding Generation</h3>
<p>ESM-2 embeddings were generated using the facebook/esm2_t33_650M_UR50D model. Sequences were processed in batches of 8 with a maximum length of 1,024 amino acids. Average pooling was applied to obtain fixed-length 1,280-dimensional representations for each sequence.</p>
<h3 id="machine-learning-pipeline">Machine Learning Pipeline</h3>
<p>Seven algorithms were benchmarked: XGBoost, Random Forest, SVM-RBF, Logistic Regression, MLP, KNN, and Perceptron. All models included:
- StandardScaler preprocessing
- Class imbalance handling (scale_pos_weight for XGBoost, class_weight='balanced' for others)
- 5-fold stratified cross-validation
- Randomized hyperparameter search (20 iterations)
- Comprehensive evaluation metrics</p>
<h3 id="traditional-methods">Traditional Methods</h3>
<p>Four traditional bioinformatics approaches were implemented:
- <strong>Sequence Similarity</strong>: Based on pairwise sequence identity
- <strong>Motif-based</strong>: Using conserved terpene synthase motifs (DDXXD, NSE/DTE, RRX8W, GXGXG)
- <strong>Length-based</strong>: Using sequence length as the primary feature
- <strong>Amino Acid Composition</strong>: Using 20-dimensional AA frequency vectors</p>
<h3 id="statistical-analysis_1">Statistical Analysis</h3>
<p>Performance differences were assessed using paired t-tests with significance threshold p &lt; 0.001. Confidence intervals (95%) were calculated for all performance metrics.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This comprehensive benchmark demonstrates that ESM-2 protein language model embeddings combined with machine learning algorithms provide a powerful and robust approach for terpene synthase classification. Our multi-product analysis reveals that while performance varies with class balance and target product, ESM-2 + ML approaches consistently outperform traditional bioinformatics methods. The framework established here can be readily extended to other enzyme families and provides a foundation for future computational enzyme discovery efforts.</p>
<h2 id="data-availability">Data Availability</h2>
<p>All code, data, and results are available at: https://github.com/ah474747/terpene-synthase-classification</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>We thank the MARTS-DB database curators for providing the gold-standard dataset used in this study. We also acknowledge the computational resources provided by [institution].</p>
<h2 id="references">References</h2>
<ol>
<li>Chen, F. et al. (2011). The family of terpene synthases in plants: a mid-size family of genes for specialized metabolism that is highly diversified throughout the kingdom. Plant J. 66, 212-229.</li>
<li>Christianson, D.W. (2017). Structural and chemical biology of terpenoid cyclases. Chem. Rev. 117, 11570-11648.</li>
<li>Radivojac, P. et al. (2013). A large-scale evaluation of computational protein function prediction. Nat. Methods 10, 221-227.</li>
<li>Cane, D.E. (1999). Sesquiterpene biosynthesis: cyclization mechanisms. In Comprehensive Natural Products Chemistry, Barton, D., Nakanishi, K., and Meth-Cohn, O., eds. (Oxford: Elsevier), pp. 155-200.</li>
<li>Lin, Z. et al. (2023). Evolutionary-scale prediction of atomic-level protein structure with a language model. Science 379, 1123-1130.</li>
</ol>
    </body>
    </html>
    