{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Generate ESM-2 Embeddings (Google Colab - GPU Required)\n",
        "\n",
        "**Purpose**: Generate ESM-2 embeddings for UniProt terpene synthase sequences\n",
        "\n",
        "**Runtime**: ~30-60 minutes for 5000 sequences on Colab T4 GPU\n",
        "\n",
        "**Instructions**:\n",
        "1. Upload `uniprot_tps_sequences.fasta` from Step 1\n",
        "2. Run all cells\n",
        "3. Download `uniprot_tps_embeddings.npy` for local prediction\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q fair-esm torch transformers biopython tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify GPU Access\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úì GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚úó WARNING: No GPU detected. This will be very slow!\")\n",
        "    print(\"  Go to Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload FASTA File\n",
        "\n",
        "Click the file icon (üìÅ) on the left sidebar and upload `uniprot_tps_sequences.fasta`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if FASTA file is uploaded\n",
        "import os\n",
        "\n",
        "fasta_file = 'uniprot_tps_sequences.fasta'\n",
        "\n",
        "if os.path.exists(fasta_file):\n",
        "    print(f\"‚úì Found FASTA file: {fasta_file}\")\n",
        "    print(f\"  File size: {os.path.getsize(fasta_file) / 1e6:.2f} MB\")\n",
        "else:\n",
        "    print(f\"‚úó FASTA file not found: {fasta_file}\")\n",
        "    print(\"  Please upload the file using the file browser\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load ESM-2 Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, EsmModel\n",
        "\n",
        "print(\"Loading ESM-2 model (650M parameters)...\")\n",
        "print(\"This may take 2-3 minutes...\")\n",
        "\n",
        "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = EsmModel.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úì Model loaded on {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse FASTA File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from Bio import SeqIO\n",
        "from typing import List, Tuple\n",
        "\n",
        "def parse_fasta(fasta_file: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Parse FASTA file and return list of (header, sequence) tuples.\"\"\"\n",
        "    sequences = []\n",
        "    for record in SeqIO.parse(fasta_file, 'fasta'):\n",
        "        header = record.id\n",
        "        sequence = str(record.seq)\n",
        "        sequences.append((header, sequence))\n",
        "    return sequences\n",
        "\n",
        "print(f\"Parsing {fasta_file}...\")\n",
        "sequences = parse_fasta(fasta_file)\n",
        "print(f\"‚úì Loaded {len(sequences)} sequences\")\n",
        "print(f\"  Average length: {sum(len(seq) for _, seq in sequences) / len(sequences):.0f} amino acids\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def generate_embeddings(sequences: List[Tuple[str, str]], \n",
        "                       model, \n",
        "                       tokenizer, \n",
        "                       device,\n",
        "                       batch_size: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate ESM-2 embeddings for sequences.\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: numpy array of shape (n_sequences, 1280)\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(sequences)} sequences...\")\n",
        "    print(f\"Using batch size: {batch_size}\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for header, sequence in tqdm(sequences, desc=\"Processing sequences\"):\n",
        "            try:\n",
        "                # Tokenize\n",
        "                inputs = tokenizer(\n",
        "                    sequence,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=False,\n",
        "                    truncation=True,\n",
        "                    max_length=1024\n",
        "                )\n",
        "                \n",
        "                # Move to device\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                \n",
        "                # Generate embedding\n",
        "                outputs = model(**inputs)\n",
        "                \n",
        "                # Extract mean pooled embedding\n",
        "                # Shape: (1, seq_len, 1280) -> (1280,)\n",
        "                embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "                \n",
        "                embeddings.append(embedding)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing {header}: {e}\")\n",
        "                print(f\"Sequence length: {len(sequence)}\")\n",
        "                # Use zero embedding as fallback\n",
        "                embeddings.append(np.zeros(1280))\n",
        "    \n",
        "    # Convert to numpy array\n",
        "    embeddings = np.array(embeddings)\n",
        "    print(f\"‚úì Generated embeddings shape: {embeddings.shape}\")\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = generate_embeddings(sequences, model, tokenizer, device, batch_size=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = 'uniprot_tps_embeddings.npy'\n",
        "\n",
        "print(f\"Saving embeddings to {output_file}...\")\n",
        "np.save(output_file, embeddings)\n",
        "\n",
        "print(f\"‚úì Embeddings saved!\")\n",
        "print(f\"  File: {output_file}\")\n",
        "print(f\"  Size: {os.path.getsize(output_file) / 1e6:.2f} MB\")\n",
        "print(f\"  Shape: {embeddings.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EMBEDDING GENERATION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Download 'uniprot_tps_embeddings.npy' (right-click > Download)\")\n",
        "print(\"2. Move it to your local 'data/' directory\")\n",
        "print(\"3. Run step3_predict_germacrene.py locally\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Verify Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick sanity check\n",
        "print(\"Embedding statistics:\")\n",
        "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
        "print(f\"  Std: {embeddings.std():.4f}\")\n",
        "print(f\"  Min: {embeddings.min():.4f}\")\n",
        "print(f\"  Max: {embeddings.max():.4f}\")\n",
        "print(f\"  Contains NaN: {np.isnan(embeddings).any()}\")\n",
        "print(f\"  Contains Inf: {np.isinf(embeddings).any()}\")\n",
        "\n",
        "if np.isnan(embeddings).any() or np.isinf(embeddings).any():\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: Embeddings contain NaN or Inf values!\")\n",
        "else:\n",
        "    print(\"\\n‚úì Embeddings look good!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
